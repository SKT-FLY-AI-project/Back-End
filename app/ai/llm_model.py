import torch
from transformers import AutoModelForVision2Seq, AutoProcessor

class LLMModel:
    def __init__(self):
        # self.model_name = "Qwen/Qwen2-VL-2B-Instruct"
        self.model_name = "Qwen/Qwen2-VL-2B-Instruct"
        self.device = "cuda"
        self.model = None
        self.processor = None

    def load_model(self):
        """FastAPI ì‹¤í–‰ ì‹œ í•œ ë²ˆë§Œ ëª¨ë¸ì„ ë¡œë“œ"""
        if self.model is None:
            print("ğŸ”¹ LLM ëª¨ë¸ ë¡œë“œ ì¤‘...")
            torch.cuda.empty_cache()
            self.model = AutoModelForVision2Seq.from_pretrained(
                self.model_name,
                torch_dtype=torch.float16,
                device_map="auto"
            )
            self.processor = AutoProcessor.from_pretrained(self.model_name)
            print("âœ… LLM ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

    def get_model(self):
        """ë¡œë“œëœ ëª¨ë¸ì„ ë°˜í™˜"""
        if self.model is None or self.processor is None:
            self.load_model()
        return self.model, self.processor
    
llm_model = LLMModel()