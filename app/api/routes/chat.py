from fastapi import APIRouter, HTTPException, Body
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import List, Dict, Optional
from groq import Groq
from app.models.llm_model import LLMModel
from app.utils.llm_utils import generate_vlm_description_qwen, generate_rich_description
import os
import json
import time

router = APIRouter(prefix="/chat", tags=["Chat"])
os.environ['HF_HOME'] = "D:/huggingface_models"
client = Groq(api_key="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")

class ChatMessage(BaseModel):
    """VTS ëŒ€í™” ì„¸ì…˜ ìš”ì²­ ë°ì´í„° ëª¨ë¸"""
    request: str
    # conversation_history: Optional[List[Dict[str, str]]] = None
    image_title: str
    vlm_description: str
    dominant_colors: List

class SessionRequest(BaseModel):
    """ëŒ€í™” ì„¸ì…˜ ê´€ë¦¬ ìš”ì²­ ëª¨ë¸"""
    session_id: str


# ì„¸ì…˜ ë§Œë£Œ ì‹œê°„ ì„¤ì • (ì´ˆ)
SESSION_TIMEOUT = 3600  # 1ì‹œê°„

# ëŒ€í™” ì„¸ì…˜ ì €ì¥ ë”•ì…”ë„ˆë¦¬
conversation_sessions = {}


def clear_expired_sessions():
    """ì˜¤ë˜ëœ ì„¸ì…˜ì„ ì •ë¦¬í•˜ëŠ” í•¨ìˆ˜"""
    current_time = time.time()
    expired_sessions = [
        session_id
        for session_id, data in conversation_sessions.items()
        if current_time - data["last_activity"] > SESSION_TIMEOUT
    ]

    for session_id in expired_sessions:
        del conversation_sessions[session_id]
        print(f"ğŸ—‘ï¸ ì˜¤ë˜ëœ ì„¸ì…˜ {session_id} ì‚­ì œë¨")


@router.post("/create_session/{userid}/{photo_id}")
async def create_session(userid: str, photo_id: str):
    """ìƒˆë¡œìš´ ëŒ€í™” ì„¸ì…˜ì„ ìƒì„±í•˜ëŠ” ì—”ë“œí¬ì¸íŠ¸"""
    session_id = f"{userid}_{photo_id}"

    # ê¸°ì¡´ ì„¸ì…˜ í™•ì¸
    if session_id in conversation_sessions:
        return {
            "session_id": session_id,
            "message": "ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ì„¸ì…˜ì´ ìˆìŠµë‹ˆë‹¤.",
            "conversation": conversation_sessions[session_id]["messages"],
        }

    # ìƒˆë¡œìš´ ì„¸ì…˜ ìƒì„±
    conversation_sessions[session_id] = {"messages": [], "last_activity": time.time()}

    return {"session_id": session_id, "message": "ìƒˆë¡œìš´ ì„¸ì…˜ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤."}


@router.get("/check_session/{userid}/{photo_id}")
async def check_session(userid: str, photo_id: str):
    """ì„¸ì…˜ ì¡´ì¬ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ëŠ” ì—”ë“œí¬ì¸íŠ¸"""
    session_id = f"{userid}_{photo_id}"
    if session_id in conversation_sessions:
        return {
            "exists": True,
            "session_id": session_id,
            "conversation": conversation_sessions[session_id]["messages"],
        }
    return {"exists": False, "message": "ì„¸ì…˜ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."}


@router.post("/end_session")
async def end_session(request: SessionRequest):
    """ëŒ€í™” ì„¸ì…˜ì„ ì¢…ë£Œí•˜ëŠ” ì—”ë“œí¬ì¸íŠ¸"""
    session_id = request.session_id
    if session_id in conversation_sessions:
        del conversation_sessions[session_id]
        return {"message": f"ì„¸ì…˜ {session_id}ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."}

    raise HTTPException(status_code=404, detail="ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


def load_vts_questions():
    # í˜„ì¬ íŒŒì¼(llm.py)ì´ ìˆëŠ” í´ë” ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
    current_dir = os.path.dirname(os.path.abspath(__file__))  # three_llm í´ë” ê²½ë¡œ

    # JSON íŒŒì¼ ê²½ë¡œ ì„¤ì •
    file_path = os.path.join(current_dir, "data", "VTS_RAG_questions.json")

    # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"âŒ RAG ì§ˆë¬¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")

    with open(file_path, "r", encoding="utf-8") as f:
        return json.load(f)


# 1-2. ë¯¸ìˆ  ì •ë³´ RAGì—ì„œ ì§ˆë¬¸ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ (ì˜ˆì‹œ) : art_RAG_questions.json
def load_art_questrion(query):
    """
    ì‚¬ìš©ìì˜ ê°ìƒ ë° ì§ˆë¬¸ì— ëŒ€í•´ ê´€ë ¨ ë¯¸ìˆ  ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ëŠ” í•¨ìˆ˜.
    """
    # ì˜ˆì‹œì„. # art_RAG_question.json íŒŒì¼ ê°€ì ¸ì˜¤ëŠ” ì½”ë“œë¡œ ìˆ˜ì •í•˜ì.
    # retriever = FAISS.load_local("art_database", embeddings).as_retriever()
    # results = retriever.get_relevant_documents(query)
    # return results[0].page_content if results else "ê´€ë ¨ëœ ë¯¸ìˆ  ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."


# 2. ì§ˆë¬¸ ìœ í˜• íŒŒì•…í•˜ì—¬ -> ì•Œë§ì€ ì§ˆë¬¸ í˜•ì„±í•˜ê¸°
def retrieve_question(
    user_requests, image_title, vlm_description, dominant_colors, edges=[]
):
    """
    ì‚¬ìš©ìì˜ ì´ì „ ë‹µë³€ì„ ê¸°ë°˜ìœ¼ë¡œ ì ì ˆí•œ 'VTS ì§ˆë¬¸'ì„ RAGì—ì„œ ê²€ìƒ‰
    """
    """
    - ì‚¬ìš©ìì˜ ì…ë ¥ì„ ë¶„ì„í•˜ì—¬ VTS ì§ˆë¬¸ì„ ìƒì„±í• ì§€, ë¯¸ìˆ  ì •ë³´ë¥¼ ì œê³µí• ì§€ ê²°ì •.
    - ê°ì •ì  ê³µê° ë˜ëŠ” ì •ë³´ ì œê³µì´ í•„ìš”í•œ ê²½ìš°: ë¯¸ìˆ  ì •ë³´ ê²€ìƒ‰
    - ì§ˆë¬¸ì„ ìƒì„±í•  ê²½ìš°: VTS ì§ˆë¬¸ ë§¤ë‰´ì–¼ ê²€ìƒ‰
    """
    # ì‚¬ìš©ìì˜ ë§ˆì§€ë§‰ ì§ˆë¬¸ (AIê°€ ì§ˆë¬¸ ìƒì„±ì„ ìœ„í•´ ë„£ì–´ì¤˜ì•¼í•  ê°’)
    print(user_requests)
    previous_responses = user_requests[-1]
    
    rag_questions = (
        load_vts_questions()
    )  # ì¼ë‹¨ í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì—¬ê¸°ë‹¤ ë‘ê¸´ í•˜ëŠ”ë°... ë‚˜ì¤‘ì— ì •ë¦¬í•©ì‹œë‹¤.
    # RAGì—ì„œ ê²€ìƒ‰ (í˜„ì¬ëŠ” ì„ì‹œë¡œ JSONì—ì„œ ì§ˆë¬¸ì„ ì„ íƒí•˜ëŠ” í˜•íƒœ)

    # ğŸ¨ 2-1. ì‚¬ìš©ìê°€ ì‘í’ˆ ì„¤ëª…ì„ ìš”êµ¬í•¨.
    if (
        "ëŠë‚Œ" in previous_responses
        or "ì„¤ëª…" in previous_responses
        or "ë°°ê²½" in previous_responses
    ):  # NLPë¡œ ê°œì„ í•˜ê¸°
        relevant_questions = []
        # ğŸ‘‰ RAG : ì‘í’ˆ ì •ë³´ ì„¤ëª…í•˜ê¸°.
        relevant_questions = load_art_questrion(previous_responses)
        # ğŸ‘‰ RAG : VTSì˜ ì§ˆë¬¸ì„ ì „ë‹¬.
        relevant_questions = [q for q in rag_questions if q["classification"] == "ì§ˆë¬¸"]

        print(f"ğŸ“š ART AI LLM : {relevant_questions}")
        return relevant_questions

    # ğŸ¨ 2-2. ì‚¬ìš©ìê°€ ìì‹ ì˜ ìƒê°ì„ ë§í•¨.
    else:
        relevant_questions = []
        # ğŸ‘‰ RAG : : VTSì˜ ë°˜ì‘ì™€ ê´€ë ¨ëœ ë§ì„ ì „ë‹¬.
        # ì‚¬ìš©ìì˜ ì´ì „ ë‹µë³€ì„ ë¶„ì„ (ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœíˆ ëœë¤ìœ¼ë¡œ ì„ íƒ, ì‹¤ì œ êµ¬í˜„ ì‹œ NLP í™œìš© ê°€ëŠ¥)
        relevant_questions = [q for q in rag_questions if q["classification"] == "ë°˜ì‘"]

        # ğŸ‘‰ LLM : AIê°€ ì‘í’ˆì— ëŒ€í•´ ìƒê°í•˜ëŠ” ë§ì„ ì „ë‹¤. + LLMê¸°ë°˜ ì‘í’ˆ ì •ë³´
        prompt = f"""
        ì‚¬ìš©ìì™€ '{image_title}' ì‘í’ˆì— ëŒ€í•´ ëŒ€í™”í•˜ê³  ìˆìŠµë‹ˆë‹¤.
        ì‘í’ˆ ì„¤ëª…: {vlm_description}
        ì£¼ìš” ìƒ‰ìƒ: {dominant_colors}
        ì—£ì§€ ê°ì§€ ê²°ê³¼: {edges}
        
        ì‚¬ìš©ìì˜ ìƒê°: "{previous_responses}"
        
        ìœ„ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ìƒê°ì— ëŒ€í•´ ìœ ìµí•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
        ì‚¬ìš©ì ìƒê°ì—ëŒ€í•´ ë™ì˜ ë° ë‹¤ë¥¸ ì˜ê²¬ì„ ì œì‹œí•´ì£¼ì„¸ìš”.
        """

        # LLMì„ ì´ìš©í•œ ë‹µë³€ ìƒì„±
        # answer = generate_rich_description(image_title, prompt, dominant_colors, edges)
        answer = generate_rich_description(image_title, prompt, dominant_colors)
        print("\nğŸ’¬ AIì˜ ë‹µë³€:")
        print(answer)

        # ğŸ‘‰ RAG : VTSì˜ ì§ˆë¬¸ì„ ì „ë‹¬.
        # ì²« ì§ˆë¬¸ì€ "ì „ì²´ì— ëŒ€í•œ ì ê·¹ì ì¸ ê´€ê³„ ë§Œë“¤ê¸°"ì—ì„œ ì„ íƒ
        relevant_questions = [q for q in rag_questions if q["classification"] == "ì§ˆë¬¸"]
        return relevant_questions


def generate_vts_response(user_input, conversation_history):
    """
    ì‚¬ìš©ìì˜ ì…ë ¥ê³¼ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì ì ˆí•œ ë°˜ì‘ê³¼ ì§ˆë¬¸ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜.
    """
    # ğŸ”¹ ëŒ€í™” ë§¥ë½ ì •ë¦¬
    context = "\n".join(conversation_history[-3:])  # ìµœê·¼ 3ê°œë§Œ ìœ ì§€ (ë©”ëª¨ë¦¬ ìµœì í™”)

    prompt = f"""
    ì‚¬ìš©ìê°€ ë¯¸ìˆ  ì‘í’ˆì„ ê°ìƒí•˜ê³  ìˆìŠµë‹ˆë‹¤.
    ì´ì „ ëŒ€í™”:
    {context}

    ì‚¬ìš©ìì˜ ì…ë ¥:
    "{user_input}"

    AIì˜ ì—­í• :
    1. ì‚¬ìš©ìì˜ ê°ìƒì— ëŒ€í•´ ì ì ˆí•œ ë°˜ì‘ì„ ì œê³µí•©ë‹ˆë‹¤.
    2. ìƒˆë¡œìš´ ì§ˆë¬¸ì„ ìƒì„±í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”ë¥¼ ì´ì–´ê°‘ë‹ˆë‹¤.

    AIì˜ ì‘ë‹µ í˜•ì‹:
    1. ë°˜ì‘: (ì‚¬ìš©ìì˜ ê°ìƒì„ ë°˜ì˜í•œ í”¼ë“œë°±)
    2. ì§ˆë¬¸: (VTS ê¸°ë°˜ì˜ ì ì ˆí•œ ì¶”ê°€ ì§ˆë¬¸)
    """

    completion = client.chat.completions.create(
        model="qwen-2.5-coder-32b",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.5,
        max_tokens=150,
        top_p=0.95
    )

    response = completion.choices[0].message.content.strip()
    
    # ğŸ”¹ ì‘ë‹µì„ "ë°˜ì‘ + ì§ˆë¬¸"ìœ¼ë¡œ ë¶„ë¦¬
    try:
        response_parts = response.split("\n")
        reaction = response_parts[0].strip() if response_parts else "í¥ë¯¸ë¡œìš´ ìƒê°ì´ì—ìš”."
        question = response_parts[1].strip() if len(response_parts) > 1 else "ì´ ì‘í’ˆì„ ë³´ê³  ì–´ë–¤ ì ì´ ê°€ì¥ ì¸ìƒì ì´ì—ˆë‚˜ìš”?"
    except:
        reaction, question = response, "ì´ ì‘í’ˆì„ ë³´ê³  ì–´ë–¤ ì ì´ ê°€ì¥ ì¸ìƒì ì´ì—ˆë‚˜ìš”?"

    return reaction[7:], question[7:]


@router.post("/bot/{userid}")
async def start_vts_conversation(userid: str, chat_data: ChatMessage = Body(...)):
    """VTS ë°©ì‹ì˜ ê°ìƒ ëŒ€í™”ë¥¼ ì§„í–‰í•˜ëŠ” ì—”ë“œí¬ì¸íŠ¸"""
    try:
        request = chat_data.request
        session_id = f"{userid}/{chat_data.image_title}"

        # ê¸°ì¡´ ì„¸ì…˜ì´ ìˆìœ¼ë©´ ë¶ˆëŸ¬ì˜¤ê¸°, ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
        if session_id in conversation_sessions:
            session_data = conversation_sessions[session_id]
        else:
            session_data = {"messages": [], "last_activity": time.time()}
            conversation_sessions[session_id] = session_data

        # ê¸°ì¡´ ëŒ€í™” ê¸°ë¡ ë¶ˆëŸ¬ì˜¤ê¸° (ì„¸ì…˜ì´ ìˆìœ¼ë©´)
        conversation_history = session_data["messages"]
        
        # ìƒˆë¡œìš´ ë©”ì‹œì§€ ì¶”ê°€
        conversation_history.append({"role": "user", "content": request})
        user_requests = [msg["content"] for msg in conversation_history]  # ë©”ì‹œì§€ì—ì„œ contentë§Œ ì¶”ì¶œ

        # LLMì„ ì‚¬ìš©í•œ ì‘ë‹µ ìƒì„±
        reaction, question = generate_vts_response(request, user_requests)
        response = reaction + '\n' + question

        # ì‘ë‹µì„ ëŒ€í™” ê¸°ë¡ì— ì €ì¥
        conversation_history.append({"role": "assistant", "content": response})
        session_data["last_activity"] = time.time()  # ì„¸ì…˜ ìµœì‹ í™”

        return {
            "session_id": session_id,
            "response": response,
            "conversation": conversation_history,  # ê¸°ì¡´ ê¸°ë¡ í¬í•¨
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì„œë²„ ì˜¤ë¥˜: {str(e)}")