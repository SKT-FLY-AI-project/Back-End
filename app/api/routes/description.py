from fastapi import APIRouter, UploadFile, File, Request
from fastapi.responses import StreamingResponse
import asyncio
from pydantic import BaseModel
from PIL import Image
import numpy as np
from app.config import UPLOAD_DIR
from app.utils.opencv_utils import load_and_preprocess_image, detect_edges, extract_dominant_colors, detect_painting_region
from app.utils.cnn_utils import predict_image
from app.utils.llm_utils import generate_vlm_description_qwen, generate_rich_description, text_to_speech
from app.utils.s3_utils import upload_to_s3

import shutil
import json
import time
import sys
import os
import cv2

# λ¶„μ„ κ²°κ³Όλ¥Ό λ¬¶λ” ν΄λμ¤ μ •μ
class ImageAnalysisResult(BaseModel):
    vlm_description: str  # μ΄λ―Έμ§€μ— λ€ν• μ„¤λ…
    dominant_colors: list  # μ£Όμ” μƒ‰μƒ
    edges: list  # μ—£μ§€ κ°μ§€ κ²°κ³Ό

class ChatRequest(BaseModel):
    user_question: str  # μ‚¬μ©μκ°€ μ…λ ¥ν• μ§λ¬Έ
    
class MessageRequest(BaseModel):
    message: str

router = APIRouter(prefix="/chat", tags=["Chatbot"])

sys.path.append(os.path.dirname(os.path.abspath(__file__)))

processing_status = {}  # μ§„ν–‰ μƒνƒ μ €μ¥
@router.get("/status/{userid}")
async def get_status(userid: str):
    """
    νΉμ • μ‚¬μ©μμ μ§„ν–‰ μƒνƒ λ°ν™
    """
    return {"status": processing_status.get(userid, "λ€κΈ° μ¤‘")}

@router.post("/describe/{userid}")
async def describe_image(userid: str, file: UploadFile = File(...)):
    """
    μ΄λ―Έμ§€λ¥Ό μ—…λ΅λ“ν•λ©΄ λ¶„μ„μ„ μν–‰ν•κ³  κ²°κ³Όλ¥Ό λ°ν™ν•λ” API μ—”λ“ν¬μΈνΈ.
    """
    # yield "START|μ΄λ―Έμ§€ μ—…λ΅λ“ μ¤‘...\n"
    file_name = file.filename
    file_path = f"{UPLOAD_DIR}/{file.filename}"

    # νμΌ μ €μ¥
    with open(file_path, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)
    file.file.close()
    file = open(file_path, "rb")
    
    async def process_image():
        print(1)
        yield json.dumps({"status": "μ΄λ―Έμ§€ μ „μ²λ¦¬ μ¤‘...", "completed": False}) + "\n"
        image_pre = load_and_preprocess_image(file_path)
        image = detect_painting_region(image_pre)
        edges = detect_edges(image)
        dominant_colors = extract_dominant_colors(image)

        print(2)
        yield json.dumps({"status": "μ΄λ―Έμ§€ μ €μ¥ λ° μ—…λ΅λ“ μ¤‘...", "completed": False}) + "\n"
        processed_file_path = f"{UPLOAD_DIR}/processed_{file_name}"
        cv2.imwrite(processed_file_path, cv2.cvtColor(image, cv2.COLOR_RGB2BGR))
        s3_url = upload_to_s3(userid, processed_file_path, f"{userid}/processed_{file_name}")

        print(3)
        yield json.dumps({"status": "μ„¤λ… μƒμ„± μ¤‘...", "completed": False}) + "\n"
        vlm_description = generate_vlm_description_qwen(file_path)
        if isinstance(vlm_description, str):
            vlm_description = [vlm_description]
        if not vlm_description:
            vlm_description = ["μ„¤λ…μ„ μƒμ„±ν•  μ μ—†μµλ‹λ‹¤."]

        print(4)
        yield json.dumps({"status": "μ‘ν’ μ λ© λ¶„μ„ μ¤‘...", "completed": False}) + "\n"
        title = predict_image(image)
        if isinstance(title, set):
            title = list(title)[0]
        else:
            title = "μ λ© μ—†μ"

        print(5)
        yield json.dumps({"status": "ν’λ¶€ν• μ„¤λ… μƒμ„± μ¤‘...", "completed": False}) + "\n"
        rich_description = generate_rich_description(title, vlm_description[0], dominant_colors, edges)

        print(6)
        yield json.dumps({"status": "μμ„± λ³€ν™ μ¤‘...", "completed": False}) + "\n"
        audio_filename = f"output_{os.path.basename(file_path)}.mp3"
        audio_path = f"uploads/{audio_filename}"
        # text_to_speech(rich_description, output_file=audio_path)

        print(7)
        final_result = {
            "status": "μ™„λ£",
            "completed": True,
            "data": {
                "image_url": s3_url,
                "title": title,
                "vlm_description": vlm_description[0],
                "rich_description": rich_description,
                "dominant_colors": dominant_colors.tolist(),
                # "edges_detected": "λ…ν™•ν νƒμ§€λ¨" if edges.sum() > 10000 else "λ¶λ…ν™•",
                "audio_url": f"/static/{audio_filename}"
            }
        }
        print(8)
        yield json.dumps(final_result) + "\n"
    print(9)
    return StreamingResponse(process_image(), media_type="text/event-stream")

@router.post("/bot")
async def chatbot(request: Request):
    """
    μ‚¬μ©μκ°€ μ§λ¬Έμ„ ν•λ©΄, λ¶„μ„λ μ΄λ―Έμ§€ κ²°κ³Όλ¥Ό λ°”νƒ•μΌλ΅ λ‹µλ³€μ„ μƒμ„±ν•λ” API μ—”λ“ν¬μΈνΈ.
    """
    body = await request.json()  # μ”μ²­ λ³Έλ¬Έμ„ JSONμΌλ΅ λ³€ν™
    print("π“¥ λ°›μ€ μ”μ²­:", body)  # μ”μ²­ λ³Έλ¬Έ μ¶λ ¥

    try:
        request_data = MessageRequest(**body)  # BaseModelμ— λ§κ² λ³€ν™
    except Exception as e:
        print("β μ”μ²­ λ°μ΄ν„° λ³€ν™ μ¤λ¥:", e)
        return {"error": "μλ»λ μ”μ²­ ν•μ‹μ…λ‹λ‹¤."}

    prompt = f"""
        μ‚¬μ©μμ μ§λ¬Έ: "{request_data.message}"
        
        μ„ μ •λ³΄λ¥Ό κΈ°λ°μΌλ΅ μ‚¬μ©μμ μ§λ¬Έμ— λ€ν•΄ μƒμ„Έν•κ³  μ μµν• λ‹µλ³€μ„ μ κ³µν•μ„Έμ”.
        """
    
    answer = generate_rich_description("λ¶„μ„λ κ·Έλ¦Ό", prompt, [], [])
    
    print("\nπ’¬ AIμ λ‹µλ³€:")
    print(answer)

    return json.loads(json.dumps({"response": answer}, ensure_ascii=False))


@router.post("/user-prompt")
async def user_prompt(request: ChatRequest, analysis_result: ImageAnalysisResult):
    """
    μ‚¬μ©μκ°€ μ§λ¬Έμ„ ν•λ©΄, λ¶„μ„λ μ΄λ―Έμ§€ κ²°κ³Όλ¥Ό λ°”νƒ•μΌλ΅ λ‹µλ³€μ„ μƒμ„±ν•λ” API μ—”λ“ν¬μΈνΈ.
    """
    
    # μ „λ‹¬λ λ¶„μ„ κ²°κ³Όλ¥Ό μ΄μ©ν•΄ ν”„λ΅¬ν”„νΈ μƒμ„±
    # prompt = f"""
    #     μ‚¬μ©μλ” '{analysis_result.vlm_description}' μ‘ν’μ— λ€ν•΄ μ§λ¬Έν•κ³  μμµλ‹λ‹¤.
    #     μ‘ν’ μ„¤λ…: {analysis_result.vlm_description}
    #     μ£Όμ” μƒ‰μƒ: {analysis_result.dominant_colors}
    #     μ—£μ§€ κ°μ§€ κ²°κ³Ό: {analysis_result.edges}
        
    #     μ‚¬μ©μμ μ§λ¬Έ: "{request.user_question}"
        
    #     μ„ μ •λ³΄λ¥Ό κΈ°λ°μΌλ΅ μ‚¬μ©μμ μ§λ¬Έμ— λ€ν•΄ μƒμ„Έν•κ³  μ μµν• λ‹µλ³€μ„ μ κ³µν•μ„Έμ”.
    #     """
    prompt = f"""
        μ‚¬μ©μλ” '{analysis_result.vlm_description}' μ‘ν’μ— λ€ν•΄ μ§λ¬Έν•κ³  μμµλ‹λ‹¤.
        μ‘ν’ μ„¤λ…: {analysis_result.vlm_description}
        μ£Όμ” μƒ‰μƒ: {analysis_result.dominant_colors}
        
        μ‚¬μ©μμ μ§λ¬Έ: "{request.user_question}"
        
        μ„ μ •λ³΄λ¥Ό κΈ°λ°μΌλ΅ μ‚¬μ©μμ μ§λ¬Έμ— λ€ν•΄ μƒμ„Έν•κ³  μ μµν• λ‹µλ³€μ„ μ κ³µν•μ„Έμ”.
        """
    
    # LLMμ„ μ΄μ©ν• λ‹µλ³€ μƒμ„±
    answer = generate_rich_description("λ¶„μ„λ κ·Έλ¦Ό", prompt, analysis_result.dominant_colors, analysis_result.edges)
    print("\nπ’¬ AIμ λ‹µλ³€:")
    print(answer)

    # μμ„± λ³€ν™
    audio_filename = f"answer_{os.path.basename(analysis_result.vlm_description)}.mp3"
    audio_path = f"uploads/{audio_filename}"
    text_to_speech(answer, output_file=audio_path)
    
    # π”Ή κ²°κ³Ό JSON λ°ν™
    return {
        "image_path": analysis_result.vlm_description,
        "vlm_description": analysis_result.vlm_description,
        "rich_description": answer,
        "dominant_colors": analysis_result.dominant_colors,
        # "edges_detected": "λ…ν™•ν νƒμ§€λ¨" if sum(analysis_result.edges) > 10000 else "λ¶λ…ν™•",
        "audio_url": f"/static/{audio_filename}"  # ν”„λ΅ νΈμ—”λ“μ—μ„ μμ„± νμΌ μ ‘κ·Ό κ°€λ¥ν•λ„λ΅ URL μ κ³µ
    }
