from fastapi import APIRouter, UploadFile, File, Depends
from pydantic import BaseModel
from PIL import Image
import numpy as np
from app.config import UPLOAD_DIR
from app.utils.opencv_utils import load_and_preprocess_image, detect_edges, extract_dominant_colors, detect_painting_region
from app.utils.cnn_utils import predict_image
from app.utils.llm_utils import generate_vlm_description_qwen, generate_rich_description, text_to_speech
from app.utils.s3_utils import upload_to_s3

import shutil
import sys
import os
import cv2

# ë¶„ì„ ê²°ê³¼ë¥¼ ë¬¶ëŠ” í´ë˜ìŠ¤ ì •ì˜
class ImageAnalysisResult(BaseModel):
    vlm_description: str  # ì´ë¯¸ì§€ì— ëŒ€í•œ ì„¤ëª…
    dominant_colors: list  # ì£¼ìš” ìƒ‰ìƒ
    edges: str  # ì—£ì§€ ê°ì§€ ê²°ê³¼
    user_question: str  # ì‚¬ìš©ìì˜ ì§ˆë¬¸ (ì„ íƒì )

class ChatRequest(BaseModel):
    user_question: str  # ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì§ˆë¬¸

router = APIRouter(prefix="/chat", tags=["Chatbot"])

sys.path.append(os.path.dirname(os.path.abspath(__file__)))

@router.post("/describe/{userid}")
async def describe_image(userid: str, file: UploadFile = File(...)):
    """
    ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ë©´ ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” API ì—”ë“œí¬ì¸íŠ¸.
    """
    file_path = f"{UPLOAD_DIR}/{file.filename}"
    
    # íŒŒì¼ ì €ì¥
    with open(file_path, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)
    
    print("ğŸ” ì´ë¯¸ì§€ ì „ì²˜ë¦¬")
    # ğŸ”¹ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë° OpenCV ë¶„ì„
    image_pre = load_and_preprocess_image(file_path)    # detection ëœ ì´ë¯¸ì§€ê°€ ë“¤ì–´ì˜¨ë‹¤ë©´, í•„ìš” ì—†ëŠ” ì½”ë“œ
    image = detect_painting_region(image_pre)           # detection ëœ ì´ë¯¸ì§€ê°€ ë“¤ì–´ì˜¨ë‹¤ë©´, í•„ìš” ì—†ëŠ” ì½”ë“œ
    edges = detect_edges(image)
    dominant_colors = extract_dominant_colors(image)

    # ì´ë¯¸ì§€ S3 ì—…ë¡œë“œ 
    # ğŸ”¹ detected ì´ë¯¸ì§€ë¥¼ ì €ì¥
    processed_file_path = f"{UPLOAD_DIR}/processed_{file.filename}"
    # ì´ë¯¸ì§€ ì €ì¥ ì „ì— RGB â†’ BGR ë³€í™˜ í›„ ì €ì¥
    cv2.imwrite(processed_file_path, cv2.cvtColor(image, cv2.COLOR_RGB2BGR))

    s3_url = upload_to_s3(userid, processed_file_path, f"processed_{file.filename}")

    print("ğŸ”§ Qwen2.5-VL ì„¤ëª… ìƒì„± ì¤‘...")
    # âœ… Qwen2.5-VL ì‹¤í–‰í•˜ì—¬ ì„¤ëª… ìƒì„±
    vlm_description = generate_vlm_description_qwen(file_path)

    if isinstance(vlm_description, str):
        vlm_description = [vlm_description]

    # âœ… ì„¤ëª…ì´ ì—†ì„ ê²½ìš° ê¸°ë³¸ê°’ ì„¤ì •
    if not vlm_description:
        vlm_description = ["ì„¤ëª…ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."]

    # í•´ë‹¹ ì‘í’ˆì´ AIê°€ í•™ìŠµí•œ ê²ƒì´ë©´, ì œëª©ì´ return "{class_name}"
    # í•´ë‹¹ ì‘í’ˆì´ AIê°€ í•™ìŠµí•œ ê²ƒì´ ì•„ë‹ˆë©´, return "Unknown Title"
    title = predict_image(image)
    if isinstance(title, set):
        title = list(title)[0]  # setì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ í›„ ì²« ë²ˆì§¸ ê°’ ê°€ì ¸ì˜¤ê¸°
    print("ì‘í’ˆ ì œëª© ì¶”ì¶œ ê²°ê³¼ì…ë‹ˆë‹¤.", title)

    print("ğŸ“ í’ë¶€í•œ ì„¤ëª… ìƒì„± ì¤‘...")
    # ğŸ”¹ LLMì„ í™œìš©í•œ ì„¤ëª… ìƒì„±
    rich_description = generate_rich_description(title, vlm_description[0], dominant_colors, edges)

    print("ğŸ¤ ìŒì„± ë³€í™˜ ì¤‘...")
    # ğŸ”¹ ìŒì„± ë³€í™˜ ì‹¤í–‰ (ìŒì„± íŒŒì¼ ì €ì¥)
    audio_filename = f"output_{os.path.basename(file_path)}.mp3"
    audio_path = f"uploads/{audio_filename}"
    text_to_speech(rich_description, output_file=audio_path)

    print("âœ… ë¶„ì„ ì™„ë£Œ, ê²°ê³¼ ë°˜í™˜ ì¤€ë¹„ ì™„ë£Œ")

    # ğŸ”¹ ê²°ê³¼ JSON ë°˜í™˜
    return {
        # "image_path": file_path,
        "image_url" : s3_url, 
        "vlm_description": vlm_description[0],
        "rich_description": rich_description,
        "dominant_colors": dominant_colors.tolist(),
        "edges_detected": "ëª…í™•íˆ íƒì§€ë¨" if edges.sum() > 10000 else "ë¶ˆëª…í™•",
        "audio_url": f"/static/{audio_filename}"  # í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ìŒì„± íŒŒì¼ ì ‘ê·¼ ê°€ëŠ¥í•˜ë„ë¡ URL ì œê³µ
    }
    
@router.post("/user-prompt/")
async def user_prompt(request: ChatRequest, analysis_result: ImageAnalysisResult):
    """
    ì‚¬ìš©ìê°€ ì§ˆë¬¸ì„ í•˜ë©´, ë¶„ì„ëœ ì´ë¯¸ì§€ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” API ì—”ë“œí¬ì¸íŠ¸.
    """
    
    # ì „ë‹¬ëœ ë¶„ì„ ê²°ê³¼ë¥¼ ì´ìš©í•´ í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = f"""
        ì‚¬ìš©ìëŠ” '{analysis_result.vlm_description}' ì‘í’ˆì— ëŒ€í•´ ì§ˆë¬¸í•˜ê³  ìˆìŠµë‹ˆë‹¤.
        ì‘í’ˆ ì„¤ëª…: {analysis_result.vlm_description}
        ì£¼ìš” ìƒ‰ìƒ: {analysis_result.dominant_colors}
        ì—£ì§€ ê°ì§€ ê²°ê³¼: {analysis_result.edges}
        
        ì‚¬ìš©ìì˜ ì§ˆë¬¸: "{request.user_question}"
        
        ìœ„ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ìƒì„¸í•˜ê³  ìœ ìµí•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
        """
    
    # LLMì„ ì´ìš©í•œ ë‹µë³€ ìƒì„±
    answer = generate_rich_description("ë¶„ì„ëœ ê·¸ë¦¼", prompt, analysis_result.dominant_colors, analysis_result.edges)
    print("\nğŸ’¬ AIì˜ ë‹µë³€:")
    print(answer)

    # ìŒì„± ë³€í™˜
    audio_filename = f"answer_{os.path.basename(analysis_result.vlm_description)}.mp3"
    audio_path = f"uploads/{audio_filename}"
    text_to_speech(answer, output_file=audio_path)
    
    # ğŸ”¹ ê²°ê³¼ JSON ë°˜í™˜
    return {
        "image_path": analysis_result.vlm_description,
        "vlm_description": analysis_result.vlm_description,
        "rich_description": answer,
        "dominant_colors": analysis_result.dominant_colors,
        "edges_detected": "ëª…í™•íˆ íƒì§€ë¨" if sum(analysis_result.edges) > 10000 else "ë¶ˆëª…í™•",
        "audio_url": f"/static/{audio_filename}"  # í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ìŒì„± íŒŒì¼ ì ‘ê·¼ ê°€ëŠ¥í•˜ë„ë¡ URL ì œê³µ
    }
