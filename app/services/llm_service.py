
import os
import json
from PIL import Image
import numpy as np
import torch
from langchain.prompts import PromptTemplate
from sentence_transformers import SentenceTransformer, util
import asyncio
import pickle

from app.utils.image_processing import get_color_name
from app.services.model_loader import llm_model  # ë¯¸ë¦¬ ë¡œë“œëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
from app.utils.text_processing import clean_and_restore_spacing
from app.config import client, PICKLE_PATH

embedding_model = SentenceTransformer("snunlp/KR-SBERT-V40K-klueNLI-augSTS")
import sys

async def generate_vlm_description_qwen(image_path):
    model, processor = llm_model.get_model()  # ë¡œë“œëœ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
    image = Image.open(image_path).convert("RGB").resize((512, 512))

    prompt = """
    ì´ ê·¸ë¦¼ì„ ë³´ê³  ì¥ë©´ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.  
    ë‹¨ìˆœí•œ í‚¤ì›Œë“œê°€ ì•„ë‹ˆë¼, ì‹¤ì œë¡œ ë³´ê³  ì´ì•¼ê¸°í•˜ë“¯ì´ ì§§ì€ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.  
    ê° ìš”ì†Œë¥¼ ê°œë³„ì ìœ¼ë¡œ ë‚˜ì—´í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ì „ì²´ì ì¸ ì¥ë©´ì„ ìì—°ìŠ¤ëŸ½ê²Œ ë¬˜ì‚¬í•´ì£¼ì„¸ìš”.  

    - ì–´ë–¤ ì‚¬ë¬¼ì´ ê°€ì¥ ëˆˆì— ë„ë‚˜ìš”?  
    - ì‚¬ëŒë“¤ì€ ë¬´ì—‡ì„ í•˜ê³  ìˆë‚˜ìš”?  
    - ìƒ‰ìƒê³¼ ë¹›ì˜ íë¦„ì€ ì–´ë–¤ ëŠë‚Œì„ ì£¼ë‚˜ìš”?  
    - ì „ì²´ì ì¸ ë¶„ìœ„ê¸°ëŠ” ì–´ë–»ê²Œ í‘œí˜„ë  ìˆ˜ ìˆë‚˜ìš”?  

    ë„ˆë¬´ ê¸¸ì§€ ì•Šê²Œ 2~3ë¬¸ì¥ ì •ë„ë¡œ ì„¤ëª…í•´ ì£¼ì„¸ìš”.
    ì„¤ëª…ì€ ë°˜ë“œì‹œ **í•œê¸€(ê°€-í£)ê³¼ ì˜ì–´(a-z)ë§Œ ì‚¬ìš©í•˜ì—¬ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.**
    âš ï¸ **í•œì(æ¼¢å­—)ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.** âš ï¸  
    í•œìê°€ í¬í•¨ë  ê²½ìš°, ë‹¤ì‹œ í•œê¸€ê³¼ ì˜ì–´ë¡œë§Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
    """
    
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image", "image": image},
                {"type": "text", "text": prompt},
            ],
        }
    ]
    if image is None:
        raise ValueError("Image is None!")
    
    text_input = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    print(f"Text input: {text_input}")
    print(f"Image type: {type(image)}")

    inputs = processor(text=[text_input], images=image, return_tensors="pt", padding=True)
    print(f"Inputs before moving to device: {inputs}")


    print("ğŸš€ Debugging inputs before moving to GPU:")
    sys.stdout.flush()  # ê°•ì œ ì¶œë ¥

    for key, value in inputs.items():
        if isinstance(value, torch.Tensor):
            inputs[key] = torch.nan_to_num(value, nan=0.0, posinf=1.0, neginf=0.0)

    for key, value in inputs.items():
        if isinstance(value, torch.Tensor):
            print(f"ğŸ”¹ {key} shape: {value.shape}, dtype: {value.dtype}, device: {value.device}")
            sys.stdout.flush()  # ê°•ì œ ì¶œë ¥

            print(f"   Min: {value.min()}, Max: {value.max()}")
            sys.stdout.flush()  # ê°•ì œ ì¶œë ¥

            print(f"   Any NaN? {torch.any(torch.isnan(value))}")
            sys.stdout.flush()  # ê°•ì œ ì¶œë ¥

            print(f"   Any Inf? {torch.any(torch.isinf(value))}")
            sys.stdout.flush()  # ê°•ì œ ì¶œë ¥
            print(f"   Unique values: {torch.unique(value)[:10]}")  # ìœ ë‹ˆí¬ ê°’ ì¼ë¶€ í™•ì¸
            sys.stdout.flush()  # ê°•ì œ ì¶œë ¥
    
    # ğŸš€ 2. `pixel_values` ì •ê·œí™” (ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” ë²”ìœ„ë¡œ ë³€í™˜)
    if "pixel_values" in inputs:
        print("ğŸš¨ Warning: Normalizing pixel_values")
        sys.stdout.flush()
        min_val, max_val = inputs["pixel_values"].min(), inputs["pixel_values"].max()
        
        # [-1, 1]ë¡œ ì •ê·œí™”
        inputs["pixel_values"] = 2 * ((inputs["pixel_values"] - min_val) / (max_val - min_val)) - 1

    # ğŸš€ 3. `int64` ê°’ ìœ ì§€ (ì˜ëª» ë³€í™˜ë˜ì§€ ì•Šë„ë¡)
    # ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” dtypeì— ë§ê²Œ ë³€í™˜
    inputs["input_ids"] = inputs["input_ids"].long()  # ì¼ë°˜ì ìœ¼ë¡œ long ìœ ì§€
    inputs["attention_mask"] = inputs["attention_mask"].long()
    inputs["image_grid_thw"] = inputs["image_grid_thw"].long()
    
    # ë§Œì•½ `images` ê°’ì´ í¬í•¨ëœë‹¤ë©´ float32ë¡œ ë³€í™˜
    if "images" in inputs:
        inputs["images"] = inputs["images"].float()
    
    # ğŸš€ 4. GPUë¡œ ì´ë™ (`pixel_values`ë§Œ `float32` ë³€í™˜)
    device = model.device
    inputs = {
        k: v.to(device, dtype=torch.float32) if k == "pixel_values" else v.to(device)
        for k, v in inputs.items() if isinstance(v, torch.Tensor)
    }
    
    print("âœ… Successfully moved inputs to GPU.")
    sys.stdout.flush()


    loop = asyncio.get_event_loop()
    with torch.no_grad():
        outputs = await loop.run_in_executor(
            None, lambda: model.generate(**inputs, max_new_tokens=256)  # 512 â†’ 256ìœ¼ë¡œ ë³€ê²½
        )


    description = processor.batch_decode(outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
    description = clean_and_restore_spacing(processor.batch_decode(outputs, skip_special_tokens=True)[0])
    
    return description

async def generate_rich_description(title, artist, correct_period, vlm_desc, dominant_colors, edges=[]):
    """
    AIê°€ ìƒì„±í•œ ê¸°ë³¸ ì„¤ëª…ì„ ê¸°ë°˜ìœ¼ë¡œ ë³´ë‹¤ í’ë¶€í•œ ê·¸ë¦¼ ì„¤ëª…ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜.
    """
    color_names = [get_color_name(c) for c in dominant_colors[:5]]
    unique_colors = list(dict.fromkeys([color.strip() for name in color_names for color in name.split(',')]))
    colors_text = ", ".join(unique_colors)

    prompt_variables = {
        "title": title,
        "artist": artist,
        "correct_period": correct_period,
        "vlm_desc": vlm_desc,
        "dominant_colors": colors_text,
        "edges_detected": "ëª…í™•íˆ íƒì§€ë¨" if np.sum(edges) > 10000 else "ë¶ˆëª…í™•í•˜ê²Œ íƒì§€ë¨",
        "correct_period": correct_period
    }
    
    prompt_template = ""
    if artist is None:
        prompt_template = PromptTemplate(
            input_variables=["vlm_desc", "dominant_colors", "edges_detected"],
            template="""
            ì´ ê·¸ë¦¼ì„ ë³´ë©´ ì–´ë–¤ ëŠë‚Œì´ ë“œë‚˜ìš”?  
            ìƒ‰ê°ê³¼ ë¶„ìœ„ê¸°ê°€ ì–´ë–¤ ì¸ìƒì„ ì£¼ëŠ”ì§€ ìì—°ìŠ¤ëŸ½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.  

            - ê·¸ë¦¼ì„ ë³´ë©´ {vlm_desc} ê°™ì€ íŠ¹ì§•ì´ ìˆì–´ìš”.  
            - ìƒ‰ê°ì€ {dominant_colors} ê³„ì—´ì´ ì£¼ë¥¼ ì´ë£¨ê³  ìˆì–´ìš”.  
            - ë¹›ì˜ íë¦„ì„ ë³´ë©´ {edges_detected} ëŠë‚Œì´ì—ìš”.  

            ë„ˆë¬´ ë”±ë”±í•œ ì„¤ëª…ë³´ë‹¤ëŠ”, ì¹œêµ¬ì—ê²Œ ê·¸ë¦¼ì„ ì†Œê°œí•˜ëŠ” ëŠë‚Œìœ¼ë¡œ  
            ê°ì„±ì ì´ê³  ìì—°ìŠ¤ëŸ½ê²Œ ì´ì•¼ê¸°í•´ ì£¼ì„¸ìš”.  
            200~300ì ì •ë„ë¡œ ê°„ê²°í•˜ë©´ì„œë„ í’ë¶€í•˜ê²Œ í‘œí˜„í•´ ì£¼ì„¸ìš”.
            ì„¤ëª…ì€ ë°˜ë“œì‹œ **í•œê¸€(ê°€-í£)ê³¼ ì˜ì–´(a-z)ë§Œ ì‚¬ìš©í•˜ì—¬ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.**
            ìˆ«ì, íŠ¹ìˆ˜ë¬¸ì, í•œìëŠ” í¬í•¨í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
            """
        )
    else:
        prompt_template = PromptTemplate(
            input_variables=list(prompt_variables.keys()),
            template="""
            {title}ê³¼(ì™€) {artist}ì— ê´€í•œ ì •ë³´ë§Œ ê²€ìƒ‰í•˜ì„¸ìš”. ìƒ‰ìƒëª…({dominant_colors})ì€ ì •í™•íˆ ì£¼ì–´ì§„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
            
            "{title}"ë¼ëŠ” ì‘í’ˆì„ ê°ìƒí•˜ê³  ìˆì–´ìš”.  
            ì´ ì‘í’ˆì€ {artist}ì´(ê°€) {correct_period} ì‹œê¸°ì— ì œì‘í•œ ì‘í’ˆì´ì—ìš”.  

            - ê·¸ë¦¼ì„ ë³´ë©´ {vlm_desc} ê°™ì€ íŠ¹ì§•ì´ ìˆì–´ìš”.  
            - ìƒ‰ê°ì€ {dominant_colors} ê³„ì—´ì´ ì£¼ë¥¼ ì´ë£¨ê³  ìˆì–´ìš”. ìƒ‰ìƒ ì´ë¦„ì€ ì •í™•íˆ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ì£¼ì„¸ìš”.
            
            ì´ ì‘í’ˆì˜ ë¶„ìœ„ê¸°ì™€ ì—­ì‚¬ì  ì˜ë¯¸ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ì„¤ëª…í•´ ì£¼ì„¸ìš”.  
            ë„ˆë¬´ í•™ë¬¸ì ì¸ ì„¤ëª…ë³´ë‹¤ëŠ”, í¸ì•ˆí•œ ëŒ€í™”ì²˜ëŸ¼ í‘œí˜„í•´ ì£¼ì„¸ìš”.
            200~300ì ì •ë„ë¡œ ê°„ê²°í•˜ê³  ê°ì„±ì ìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”.
            ì„¤ëª…ì€ ë°˜ë“œì‹œ **í•œê¸€(ê°€-í£)ê³¼ ì˜ì–´(a-z)ë§Œ ì‚¬ìš©í•˜ì—¬ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.**
            ìˆ«ì, íŠ¹ìˆ˜ë¬¸ì, í•œìëŠ” í¬í•¨í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
            """
        )

    filtered_prompt_variables = {k: v for k, v in prompt_variables.items() if v is not None}

    formatted_prompt = prompt_template.format(**filtered_prompt_variables)

    completion = client.chat.completions.create(
        model="qwen-2.5-coder-32b",
        messages=[{"role": "user", "content": formatted_prompt}],
        temperature=0.5,
        max_tokens=1024,
        top_p=0.95
    )

    return completion.choices[0].message.content.strip()


def load_vts_questions():
    """VTS ì§ˆë¬¸ íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜"""
    current_dir = os.path.dirname(os.path.abspath(__file__))  
    file_path = os.path.join(current_dir, "data", "VTS_RAG_questions.json")

    if not os.path.exists(file_path):
        raise FileNotFoundError(f"âŒ VTS ì§ˆë¬¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")

    with open(file_path, "r", encoding="utf-8") as f:
        return json.load(f)
    
# âœ… ì‚¬ìš©ìì˜ ì…ë ¥ ìœ í˜• ë¶„ì„ (ì‘í’ˆ ì •ë³´ ìš”êµ¬ vs ê°ìƒ í‘œí˜„)
import re

def classify_user_input(user_input):
    """
    ë¯¸ìˆ ì‘í’ˆ ê°ìƒ ê´€ë ¨ ëŒ€í™”ë¥¼ ì •ë³´ ìš”ì²­ê³¼ ê°ìƒ í‘œí˜„ìœ¼ë¡œ ë¶„ë¥˜í•˜ëŠ” í•¨ìˆ˜
    ë” ìœ ì—°í•œ ë§¤ì¹­ê³¼ unknown ìƒí™© ì²˜ë¦¬ ê°œì„ 
    """
    # í•µì‹¬ í‚¤ì›Œë“œ (ì–´ê·¼ ì¤‘ì‹¬ìœ¼ë¡œ - í™œìš©í˜•ì„ ê³ ë ¤)
    info_keywords = [
        "ë¬´ì—‡", "ë­", "ì–´ë–¤", "ì–´ë–»", "ì–¸ì œ", "ì–´ë””", "ëˆ„ê°€", "ëˆ„êµ¬", "ì™œ",
        "ì•Œë ¤", "ì„¤ëª…", "ê°€ë¥´ì³", "ë§í•´", "ê¶ê¸ˆ", "ì˜ë¯¸", "ìƒì§•", "ì—­ì‚¬", 
        "ì‘ê°€", "í™”ê°€", "ì œëª©", "ë…„ë„", "ì‹œê¸°", "ì†Œì¥", "ê¸°ë²•", "ì–´ë””",
        "ë§Œë“ ", "ë§Œë“¤", "ê·¸ë¦°", "ê·¸ë¦¬", "ì œì‘", "ì°½ì‘", "íƒœì–´", "ì¶œìƒ",
        "ì–´ëŠ", "ì–´ë– ", "ë°©ë²•", "ë°©ì‹", "ì´ìœ ", "ìì„¸", "ì •í™•", "ìì„¸íˆ"
    ]
    
    feeling_keywords = [
        "ëŠë‚Œ", "ê°ì •", "ì¸ìƒ", "ì•„ë¦„", "ë©‹ì§€", "ì¢‹", "ë§ˆìŒ", 
        "ê°™ì•„", "ë³´ì—¬", "ëŠê»´", "ì—°ìƒ", "ìƒê°", "ë³´ì…",
        "ë“¤ì–´", "ê°ë™", "ìŠ¬", "ê¸°ì˜", "ì˜ˆì˜", "ì•„ë¦„", "í›Œë¥­",
        "ìƒ‰ê°", "ë¶„ìœ„ê¸°", "í‘œí˜„", "í„°ì¹˜", "ë§¤ë ¥", "ê°•ë ¬", "ë¶€ë“œëŸ½",
        "ì •ë§", "ì°¸", "ë„ˆë¬´", "ë§¤ìš°", "êµ‰ì¥", "ë†€ë¼", "ì¸ìƒì "
    ]
    
    # ë¬¸ì¥ íƒ€ì…ë³„ ì ìˆ˜
    info_score = 0
    feeling_score = 0
    
    # í‚¤ì›Œë“œ ìœ ì—° ë§¤ì¹­
    for kw in info_keywords:
        if kw in user_input:
            info_score += 1
    
    for kw in feeling_keywords:
        if kw in user_input:
            feeling_score += 1
    
    # ë¬¸ì¥ ëë§ºìŒ ì²´í¬ (ê°€ì¤‘ì¹˜ ë†’ê²Œ)
    if re.search(r'(\?|ê¹Œìš”\?|ë‚˜ìš”\?|ì¸ê°€ìš”\?|ë ¤ë©´\?|ì„¸ìš”\?|ì„ê¹Œìš”\?|ê°€ìš”\?)', user_input):
        info_score += 2
    
    if re.search(r'(ë„¤ìš”|ì–´ìš”|ì•„ìš”|êµ°ìš”|ìŠµë‹ˆë‹¤|ì…ë‹ˆë‹¤|ì˜ˆìš”|ì—ìš”|!|\~)', user_input):
        feeling_score += 2
    
    # unknown ìƒí™© ì²˜ë¦¬ ì „ëµ
    if info_score == 0 and feeling_score == 0:
        # 1. ì§§ì€ ë¬¸ì¥ì€ ì •ë³´ ìš”ì²­ìœ¼ë¡œ ê°„ì£¼ (ë§ì€ ì§ˆë¬¸ì´ ì§§ìŒ)
        if len(user_input) < 10:
            return "info"
        
        # 2. ë¬¸ì¥ êµ¬ì¡° ë¶„ì„ - ì˜ë¬¸ë¬¸ íŒ¨í„´
        if re.search(r'(ì´|ê°€|ì€|ëŠ”|ì—|ì´ê²Œ|ì €ê²ƒ|ì € ê·¸ë¦¼) (ë­|ë¬´ì—‡|ì–´ë””|ëˆ„êµ¬)', user_input):
            return "info"
        
        # 3. ë¶€ë¶„ ë§¤ì¹­ - ì¶”ê°€ íŒ¨í„´
        if any(pattern in user_input for pattern in ["ì‘í’ˆ", "ê·¸ë¦¼", "ë¯¸ìˆ ", "ì–¸ì œ", "ì–´ë””", "ëˆ„ê°€"]):
            return "info"
        
        # 4. ê¸°ë³¸ê°’ ì„¤ì • - ëŒ€ë¶€ë¶„ì˜ ë¯¸ìˆ ê´€ ëŒ€í™”ëŠ” ì •ë³´ ìš”ì²­ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ
        return "info"
    
    # ê¸°ë³¸ ë¶„ë¥˜ ë¡œì§
    if info_score > feeling_score:
        return "info"
    elif feeling_score > info_score:
        return "feeling"
    else:
        # ë™ì ì¸ ê²½ìš°, ì§§ì€ ë©”ì‹œì§€ëŠ” ë³´í†µ ì§ˆë¬¸ì¼ í™•ë¥ ì´ ë†’ìœ¼ë¯€ë¡œ infoë¡œ
        if len(user_input) < 15:
            return "info"
        else:
            return "mixed"

# ë¯¸ìˆ ì‘í’ˆ RAG ê´€ë ¨ í•¨ìˆ˜
# 1. í…ìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
# ë¯¸ë¦¬ ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ pickle íŒŒì¼ì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°
async def load_precomputed_data(pickle_file=None):
    """
    ì €ì¥ëœ pickle íŒŒì¼ì—ì„œ ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™€ ë°˜í™˜í•©ë‹ˆë‹¤.
    ë§Œì•½ pickle_file ì¸ìê°€ ì£¼ì–´ì§€ì§€ ì•Šìœ¼ë©´, í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ ê¸°ì¤€ìœ¼ë¡œ './data/precomputed_data.pkl' ê²½ë¡œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    # if pickle_file is None:
    #     current_dir = os.path.dirname(os.path.abspath(__file__))
    #     pickle_file = os.path.join(current_dir, "data", "precomputed_data.pkl")
        
    with open(PICKLE_PATH, 'rb') as f:
        precomputed_data = pickle.load(f)
    return precomputed_data

def find_top_k_similar(query_sentence, sentence_dict, embeddings, model, top_k=20, threshold=0.65):
    """
    query_sentenceì™€ ì„ë² ë”©ëœ ë¬¸ì¥ë“¤ ì‚¬ì´ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ì—¬,
    ìƒìœ„ top_k ê°œì˜ ë¬¸ì¥(ì¸ë±ìŠ¤, ë¬¸ì¥, ìœ ì‚¬ë„) ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì„ê³„ì¹˜ë³´ë‹¤ ë‚®ì€ ìœ ì‚¬ë„ëŠ” ê²°ê³¼ì—ì„œ ì œì™¸í•©ë‹ˆë‹¤.
    """
    query_embedding = model.encode(query_sentence, convert_to_tensor=True)
    cosine_scores = util.pytorch_cos_sim(query_embedding, embeddings)[0]
    top_k_scores, top_k_indices = torch.topk(cosine_scores, k=top_k)
    
    results = []
    for score, idx in zip(top_k_scores, top_k_indices):
        score = score.item()
        idx = idx.item()
        sentence_key = str(idx + 1)  # ì¸ë±ìŠ¤ëŠ” 1ë¶€í„° ì‹œì‘í•œë‹¤ê³  ê°€ì •
        if score >= threshold:
            results.append((sentence_key, sentence_dict[sentence_key], score))
    return results

# 2. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ í•¨ìˆ˜
def retrieve_relevant_info(precomputed_data, query, title, artist, top_k=20, threshold=0.45):
    """
    ë¯¸ë¦¬ ì „ì²˜ë¦¬ëœ ë°ì´í„°ì—ì„œ, titleê³¼ artist ì •ë³´ë¥¼ í¬í•¨í•œ ê²€ìƒ‰ ì¿¼ë¦¬ì™€ ê°€ì¥ ìœ ì‚¬í•œ ìƒìœ„ top_k ê°œì˜ ë¬¸ì¥ì„ ì°¾ì•„
    ê° ë¬¸ì¥ì˜ ì¶œì²˜ ì¸ë±ìŠ¤ì™€ í•¨ê»˜ ë¬¸ìì—´ í˜•íƒœë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    # titleê³¼ artist ì •ë³´ë¥¼ í¬í•¨í•œ í†µí•© ì¿¼ë¦¬ ìƒì„±
    combined_query = f"{title} {artist} {query}"
    
    sentence_dict = precomputed_data["sentence_dict"]
    embeddings = precomputed_data["embeddings"]
    results = find_top_k_similar(combined_query, sentence_dict, embeddings, embedding_model, top_k=top_k, threshold=threshold)
    
    if results:
        formatted_results = "\n".join([f"Source [{key}]: {sentence} (ìœ ì‚¬ë„: {sim_score:.2f})"
                                       for key, sentence, sim_score in results])
        return formatted_results
    else:
        return "ìœ ì‚¬í•œ ê´€ë ¨ ë¯¸ìˆ  ìë£Œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

def answer_user_question(user_response, conversation_history, title, artist, rich_description, precomputed_data):
    """RAGë¥¼ í™œìš©í•˜ì—¬ ë¯¸ìˆ  ì‘í’ˆ ê´€ë ¨ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” í•¨ìˆ˜"""
    # ëŒ€í™” ë§¥ë½ ì •ë¦¬
    context = "\n".join(conversation_history[-3:])  # ìµœê·¼ 3ê°œë§Œ ìœ ì§€ (ë©”ëª¨ë¦¬ ìµœì í™”)
    print(context)
    # RAG: ì§ˆë¬¸ì— ê´€ë ¨ëœ ì •ë³´ ê²€ìƒ‰
    retrieved_info = retrieve_relevant_info(precomputed_data, user_response, title, artist, top_k=20, threshold=0.45)
    print(retrieved_info)
    
    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = f"""
    ì‚¬ìš©ìëŠ” '{artist}'ì˜ '{title}' ì‘í’ˆì— ëŒ€í•´ ì§ˆë¬¸í•˜ê³  ìˆìŠµë‹ˆë‹¤.
    
    ì´ì „ ëŒ€í™”: 
    {context}
    
    ì‚¬ìš©ìì˜ ì§ˆë¬¸: 
    "{user_response}"
    
    ì‘í’ˆ ì„¤ëª…: 
    "{rich_description}"
    
    ê´€ë ¨ ë¯¸ìˆ  ìë£Œ:
    {retrieved_info}
    
    ìœ„ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìœ ìµí•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
    ê´€ë ¨ ë¯¸ìˆ  ìë£Œì—ì„œ ì°¾ì€ ì •ë³´ë¥¼ í™œìš©í•˜ë˜, ì‘í’ˆê³¼ ì§ì ‘ ê´€ë ¨ì´ ì—†ëŠ” ë‚´ìš©ì€ ì œì™¸í•˜ì„¸ìš”.
    200~300ì ì •ë„ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•´ ì£¼ì„¸ìš”.
    ì„¤ëª…ì€ ë°˜ë“œì‹œ **í•œê¸€(ê°€-í£)ê³¼ ì˜ì–´(a-z)ë§Œ ì‚¬ìš©í•˜ì—¬ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.**
    ìˆ«ì, íŠ¹ìˆ˜ë¬¸ì, í•œìëŠ” í¬í•¨í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
    **ê²€ìƒ‰í•´ì„œ ì§„ìœ„ì—¬ë¶€ê°€ í™•ì‹¤í•˜ê²Œ ê²€ì¦ëœ ë‹µë³€ë§Œ ì‘ì„±í•˜ì„¸ìš”**
    """
    
    # LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±
    completion = client.chat.completions.create(
        model="qwen-2.5-coder-32b",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.5,
        max_tokens=512,
        top_p=0.95
    )
    
    return completion.choices[0].message.content.strip()

def recommend_vts_question(user_response, previous_questions):
    """ì‚¬ìš©ìì˜ ì‘ë‹µê³¼ ê°€ì¥ ê´€ë ¨ì´ ê¹Šì€ VTS ì§ˆë¬¸ì„ ì¶”ì²œí•˜ëŠ” í•¨ìˆ˜"""
    vts_questions = load_vts_questions()
    
    # ì´ë¯¸ ì‚¬ìš©í•œ ì§ˆë¬¸ ì œì™¸
    available_questions = [q for q in vts_questions if q["question"] not in previous_questions]

    # ë¬¸ì¥ ì„ë² ë”© ìƒì„±
    user_embedding = embedding_model.encode(user_response, convert_to_tensor=True)
    question_embeddings = embedding_model.encode([q["question"] for q in available_questions], convert_to_tensor=True)

    # ìœ ì‚¬ë„ ê³„ì‚°
    similarities = util.pytorch_cos_sim(user_embedding, question_embeddings)[0]
    best_match_idx = similarities.argmax().item()

    return available_questions[best_match_idx]["question"]

def generate_vts_response(user_input, conversation_history):
    """
    ì‚¬ìš©ìì˜ ì…ë ¥ê³¼ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì ì ˆí•œ ë°˜ì‘ê³¼ ì§ˆë¬¸ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜.
    """
    # ğŸ”¹ ëŒ€í™” ë§¥ë½ ì •ë¦¬
    context = "\n".join(conversation_history[-3:])  # ìµœê·¼ 3ê°œë§Œ ìœ ì§€ (ë©”ëª¨ë¦¬ ìµœì í™”)

    prompt = f"""
    ì‚¬ìš©ìê°€ ë¯¸ìˆ  ì‘í’ˆì„ ê°ìƒí•˜ê³  ìˆìŠµë‹ˆë‹¤.
    ì´ì „ ëŒ€í™”:
    {context}

    ì‚¬ìš©ìì˜ ì…ë ¥:
    "{user_input}"

    AIì˜ ì—­í• :
    1. ì‚¬ìš©ìì˜ ê°ìƒì— ëŒ€í•´ ì ì ˆí•œ ë°˜ì‘ì„ ì œê³µí•©ë‹ˆë‹¤.
    2. ìƒˆë¡œìš´ ì§ˆë¬¸ì„ ìƒì„±í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”ë¥¼ ì´ì–´ê°‘ë‹ˆë‹¤.

    AIì˜ ì‘ë‹µ í˜•ì‹:
    1. ë°˜ì‘: (ì‚¬ìš©ìì˜ ê°ìƒì„ ë°˜ì˜í•œ í”¼ë“œë°±)
    2. ì§ˆë¬¸: (VTS ê¸°ë°˜ì˜ ì ì ˆí•œ ì¶”ê°€ ì§ˆë¬¸)
    """

    completion = client.chat.completions.create(
        model="qwen-2.5-coder-32b",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.5,
        max_tokens=150,
        top_p=0.95
    )

    response = completion.choices[0].message.content.strip()
    
    # ğŸ”¹ ì‘ë‹µì„ "ë°˜ì‘ + ì§ˆë¬¸"ìœ¼ë¡œ ë¶„ë¦¬
    try:
        response_parts = response.split("\n")
        reaction = response_parts[0].strip() if response_parts else "í¥ë¯¸ë¡œìš´ ìƒê°ì´ì—ìš”."
        question = response_parts[1].strip() if len(response_parts) > 1 else "ì´ ì‘í’ˆì„ ë³´ê³  ì–´ë–¤ ì ì´ ê°€ì¥ ì¸ìƒì ì´ì—ˆë‚˜ìš”?"
    except:
        reaction, question = response, "ì´ ì‘í’ˆì„ ë³´ê³  ì–´ë–¤ ì ì´ ê°€ì¥ ì¸ìƒì ì´ì—ˆë‚˜ìš”?"

    return reaction[7:], question[7:]