import tensorflow as tf
import torch
from transformers import AutoModelForVision2Seq, AutoProcessor
from app.config import MODEL_PATH

class CNNModel:
    def __init__(self):
        self.model = None

    def load_model(self):
        if self.model is None:
            self.model = tf.keras.models.load_model(MODEL_PATH)

    def get_model(self):
        if self.model is None:
            self.load_model()
        return self.model
    
    def __call__(self, x):
        model = self.get_model()
        return model.predict(x)
    
class LLMModel:
    def __init__(self):
        # self.model_name = "Qwen/Qwen2-VL-2B-Instruct"
        self.model_name = "Qwen/Qwen2-VL-2B-Instruct"
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.dtype = torch.float16 if torch.cuda.is_available() else torch.float32
        self.model = None
        self.processor = None

    def load_model(self):
        """FastAPI ì‹¤í–‰ ì‹œ í•œ ë²ˆë§Œ ëª¨ë¸ì„ ë¡œë“œ"""
        if self.model is None:
            print("ğŸ”¹ LLM ëª¨ë¸ ë¡œë“œ ì¤‘...")
            torch.cuda.empty_cache()
            self.model = AutoModelForVision2Seq.from_pretrained(
                self.model_name,
                torch_dtype=self.dtype,  # âœ… GPUëŠ” FP16 ì‚¬ìš©, CPUëŠ” FP32 ì‚¬ìš©
                device_map=self.device, # ì›ë˜ëŠ” auto. CPU ì“¸ê±°ë©´ cpuë¡œ ë°”ê¿”ì•¼í•¨.
                max_memory={0: "10GiB", "cpu": "30GiB"}
            )
            self.processor = AutoProcessor.from_pretrained(self.model_name)
            print("âœ… LLM ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

    def get_model(self):
        """ë¡œë“œëœ ëª¨ë¸ì„ ë°˜í™˜"""
        if self.model is None or self.processor is None:
            self.load_model()
        return self.model, self.processor
    
# ëª¨ë¸ ë¡œë“œ
cnn_model = CNNModel()
llm_model = LLMModel()