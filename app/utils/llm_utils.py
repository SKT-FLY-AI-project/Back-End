########################### SETP 2 : LLM #####################################

import openai
import os
import requests
import json
from dotenv import load_dotenv
from groq import Groq
import torch
import numpy as np
from PIL import Image

from app.utils.opencv_utils import get_color_name
from langchain.prompts import PromptTemplate

from app.models.llm_model import llm_model  # ë¯¸ë¦¬ ë¡œë“œëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
from app.services.text_processing import clean_and_restore_spacing

# Hugging Face ëª¨ë¸ ìºì‹œ ê²½ë¡œ ì„¤ì •
os.environ['HF_HOME'] = "D:/huggingface_models"
client = Groq(api_key="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")

def generate_vlm_description_qwen(image_path):
    """ë¯¸ë¦¬ ë¡œë“œëœ LLM ëª¨ë¸ì„ ì‚¬ìš©í•´ ê·¸ë¦¼ ì„¤ëª… ìƒì„±"""
    model, processor = llm_model.get_model()  # ë¡œë“œëœ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
    image = Image.open(image_path).convert("RGB").resize((512, 512))

    messages = [
        {"role": "user", "content": [{"type": "image", "image": image},
                                     {"type": "text", "text": "ì´ ê·¸ë¦¼ì„ ì„¤ëª…í•˜ì„¸ìš”."}]}
    ]
    text_input = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = processor(text=[text_input], images=image, return_tensors="pt", padding=True).to(model.device)

    with torch.no_grad():
        outputs = model.generate(**inputs, max_new_tokens=256)

    return clean_and_restore_spacing(processor.batch_decode(outputs, skip_special_tokens=True)[0])

########################### STEP 3 : í…ìŠ¤íŠ¸ ìƒì„± ë° ìŒì„± ë³€í™˜ ###############################
from gtts import gTTS

def generate_rich_description(title, vlm_desc, dominant_colors, edges):
    """
    AIê°€ ìƒì„±í•œ ê¸°ë³¸ ì„¤ëª…ì„ ê¸°ë°˜ìœ¼ë¡œ ë³´ë‹¤ í’ë¶€í•œ ê·¸ë¦¼ ì„¤ëª…ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜.
    """
    color_names = [get_color_name(c) for c in dominant_colors[:5]]
    dominant_colors_text = ", ".join(color_names)
    edges_detected = "ëª…í™•íˆ íƒì§€ë¨" if np.sum(edges) > 10000 else "ë¶ˆëª…í™•í•˜ê²Œ íƒì§€ë¨"

    prompt_template = PromptTemplate(
        input_variables=["title", "vlm_desc", "dominant_colors", "edges_detected"],
        template=f"""
        ë‹¹ì‹ ì€ ê·¸ë¦¼ ì„¤ëª… ì „ë¬¸ê°€ì…ë‹ˆë‹¤.  
        ë‹¤ìŒ ê·¸ë¦¼ì— ëŒ€í•´ ìƒì„¸í•œ ì„¤ëª…ì„ ìƒì„±í•´ì£¼ì„¸ìš”.
        ì‹œê°ì¥ì• ì¸ì—ê²Œ ì„¤ëª…í•  ìˆ˜ ìˆë„ë¡ ìì„¸í•˜ê²Œ ì‘ì„±í•´ ì£¼ì„¸ìš”.
        **ë‹¨, 200ì ~ 500ì ì‚¬ì´ì˜ ê¸¸ì´ë¡œë§Œ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤!**

        - **ì œëª©:** "{title}"  
        - **VLM ê¸°ë°˜ ê¸°ë³¸ ì„¤ëª…:** "{vlm_desc}"   

        ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê·¸ë¦¼ì— ëŒ€í•œ ìƒì„¸í•œ ì„¤ëª…ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”.  
        ì‘í’ˆì˜ ë¶„ìœ„ê¸°, ìƒ‰ì±„, êµ¬ë„, í‘œí˜„ ê¸°ë²• ë“±ì„ ë¶„ì„í•˜ê³ ,  
        ê°€ëŠ¥í•˜ë‹¤ë©´ ì—­ì‚¬ì , ì˜ˆìˆ ì  ë°°ê²½ë„ í•¨ê»˜ ì œê³µí•´ ì£¼ì„¸ìš”.  
        ì„¤ëª…ì€ ë°˜ë“œì‹œ **í•œê¸€(ê°€-í£)ë§Œ ì‚¬ìš©í•˜ì—¬ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.**  
        ì˜ì–´, ìˆ«ì, íŠ¹ìˆ˜ë¬¸ìëŠ” í¬í•¨í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.  
        """
    )

    formatted_prompt = prompt_template.format(
        title=title,
        vlm_desc=vlm_desc,
        dominant_colors=dominant_colors_text,
        edges_detected=edges_detected
    )

    completion = client.chat.completions.create(
        model="qwen-2.5-coder-32b",
        messages=[{"role": "user", "content": formatted_prompt}],
        temperature=0.5,
        max_tokens=1024,
        top_p=0.95
    )

    return completion.choices[0].message.content.strip()

def text_to_speech(text, output_file="output.mp3"):
    """
    ìƒì„±ëœ í…ìŠ¤íŠ¸ë¥¼ ìŒì„± íŒŒì¼ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥í•˜ëŠ” í•¨ìˆ˜.
    """
    try:
        if "<think>" in text:
            text = text.split("</think>")[-1].strip()
        tts = gTTS(text=text, lang='ko')
        tts.save(output_file)
        print(f"ìŒì„± íŒŒì¼ì´ '{output_file}'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        os.system(f"start {output_file}")  # Windows (macOS: open, Linux: xdg-open)
    except Exception as e:
        print(f"ìŒì„± ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


########################### STEP 4 : ì§ˆë¬¸ ë‹µë³€ ëª¨ë“œë¥¼ ì§„í–‰í•˜ëŠ” í•¨ìˆ˜ ###############################        
def answer_user_question(image_title, vlm_description, dominant_colors, edges):
    """ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë°›ì•„ LLMì„ í†µí•´ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜"""
    while True:
        user_question = input("\nâ“ ì¶”ê°€ ì§ˆë¬¸ (ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ì…ë ¥): ")
        if user_question.lower() == "exit":
            print("ğŸ“¢ ì§ˆë¬¸ ëª¨ë“œ ì¢…ë£Œ.")
            break
        
        # LLMì— ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = f"""
        ì‚¬ìš©ìëŠ” '{image_title}' ì‘í’ˆì— ëŒ€í•´ ì§ˆë¬¸í•˜ê³  ìˆìŠµë‹ˆë‹¤.
        ì‘í’ˆ ì„¤ëª…: {vlm_description}
        ì£¼ìš” ìƒ‰ìƒ: {dominant_colors}
        ì—£ì§€ ê°ì§€ ê²°ê³¼: {edges}
        
        ì‚¬ìš©ìì˜ ì§ˆë¬¸: "{user_question}"
        
        ìœ„ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ìƒì„¸í•˜ê³  ìœ ìµí•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
        """
        
        # LLMì„ ì´ìš©í•œ ë‹µë³€ ìƒì„±
        answer = generate_rich_description(image_title, prompt, dominant_colors, edges)
        print("\nğŸ’¬ AIì˜ ë‹µë³€:")
        print(answer)

        # ìŒì„± ë³€í™˜
        text_to_speech(answer, output_file=f"answer_{image_title}.mp3")
        
########################### STEP 5 : ì§ˆë¬¸ ë‹µë³€ ëª¨ë“œë¥¼ ì§„í–‰í•˜ëŠ” í•¨ìˆ˜ ###############################
def start_vts_conversation(image_title, vlm_description): # ì•„ì§ RAGëŠ” ì§„í–‰í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.
    """VTS ë°©ì‹ì˜ ê°ìƒ ëŒ€í™”ë¥¼ ì§„í–‰í•˜ëŠ” í•¨ìˆ˜"""
    print("\nğŸ–¼ï¸ VTS ê°ìƒ ëª¨ë“œ ì‹œì‘!")
    
    while True:
        # LLMì— ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = f"""
        ì‚¬ìš©ìê°€ '{image_title}' ì‘í’ˆì„ ê°ìƒí•˜ê³  ìˆìŠµë‹ˆë‹¤.
        ì‘í’ˆ ì„¤ëª…: {vlm_description}

        ì‚¬ìš©ìê°€ ë” ê¹Šì´ ê°ìƒí•  ìˆ˜ ìˆë„ë¡ VTS(Visual Thinking Strategies) ë°©ì‹ì˜ ì§ˆë¬¸ì„ í•˜ë‚˜ì”© ì œê³µí•˜ì„¸ìš”.
        ì´ì „ ì§ˆë¬¸ê³¼ ì—°ê´€ë˜ë„ë¡ ìƒˆë¡œìš´ ì§ˆë¬¸ì„ ì œì‹œí•˜ê³ , ê°ìƒìê°€ ìƒê°ì„ í™•ì¥í•  ìˆ˜ ìˆë„ë¡ ìœ ë„í•˜ì„¸ìš”.
        """
        
        # LLMì„ ì´ìš©í•œ VTS ì§ˆë¬¸ ìƒì„±
        vts_question = generate_rich_description(image_title, prompt, [], [])
        
        # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
        user_response = input(f"\nğŸ¨ {vts_question} (ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ì…ë ¥): ")
        if user_response.lower() == "exit":
            print("ğŸ“¢ VTS ê°ìƒ ëª¨ë“œ ì¢…ë£Œ.")
            break
